{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from pycorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection variable\n",
    "conn = 'mongodb://localhost:27017'\n",
    "\n",
    "# Pass connection to the pymongo instance.\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "# Connect to a database. Will create one if not already available.\n",
    "db = client.tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve a tweet from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search tweets in database using relevant keywords\n",
    "query = { \"text\": { \"$regex\": '.*market.*|.*SPX.*|.*stock.*|.*fed.*|.*econom.*|.*bull.*|.*bear.*|.*momentum.*|.*volat.|.*treasury.*|.*powell.*|.*policy.*|.*spending.*|.*tariff.*|.*trade.*|.*SPY.*|.*tax.*|.*interest.', \n",
    "                   \"$options\": 'i'}} \n",
    "\n",
    "\n",
    "tweets = db['capstone_finance'].find(query)\n",
    "\n",
    "# build a dataframe \n",
    "df = pd.DataFrame(list(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for sentiment\n",
    "df['sentiment_by_sentence'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis with CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(tweet):\n",
    "    \n",
    "    # The functional analyses a given text (a string) and returns a sentiment of every senence as a list of strings\n",
    "    \n",
    "    sentimens = []\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        result = nlp.annotate(tweet,\n",
    "                              properties={\n",
    "                                  'annotators': 'sentiment, ner, pos',\n",
    "                                  'outputFormat': 'json',\n",
    "                                  'timeout': 1000,\n",
    "                              })\n",
    "\n",
    "        sentiments = [i[\"sentiment\"] for i in result[\"sentences\"]]\n",
    "        \n",
    "    except:\n",
    "        print(\"Error\")\n",
    "        return \"nan\"\n",
    "    \n",
    "    return sentiments\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply analyze sentiment function to eery tweet in the DataFrame\n",
    "df['sentiment_by_sentence'] = df['text'].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows where sentiment_by_sentence is a nan\n",
    "\n",
    "df_clean = df[df['sentiment_by_sentence'] != 'nan']\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update MongoDB records with an array of sentence sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentiment in zip(df_clean['tweet_id'], df_clean['sentiment_by_sentence']):\n",
    "    db['capstone_finance'].update_one({'tweet_id': i}, {'$set': {'sentiment_by_sentence': sentiment}}, upsert = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to connect to and run CoreNLP server\n",
    "\n",
    "Source: https://towardsdatascience.com/natural-language-processing-using-stanfords-corenlp-d9e64c1e1024\n",
    "\n",
    "To start CoreNLP server:\n",
    "- Open command prompt\n",
    "- Go to the CoreNLP directory\n",
    "- Start server by entering the command: java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examaple code\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "text = \"This movie was actually neither that funny, nor super witty. The movie was meh. I liked watching that movie. If I had a choice, I would not watch that movie again.\"\n",
    "result = nlp.annotate(text,\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment, ner, pos',\n",
    "                       'outputFormat': 'json',\n",
    "                       'timeout': 1000,\n",
    "                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in result[\"sentences\"]:\n",
    "    print(s[\"index\"], s[\"sentiment\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford Demo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import logging\n",
    "import json\n",
    "\n",
    "class StanfordNLP:\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port,\n",
    "                                   timeout=30000)  # , quiet=False, logging_level=logging.DEBUG)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,depparse,dcoref,relation',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def word_tokenize(self, sentence):\n",
    "        return self.nlp.word_tokenize(sentence)\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        return self.nlp.pos_tag(sentence)\n",
    "\n",
    "    def ner(self, sentence):\n",
    "        return self.nlp.ner(sentence)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        return self.nlp.parse(sentence)\n",
    "\n",
    "    def dependency_parse(self, sentence):\n",
    "        return self.nlp.dependency_parse(sentence)\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return json.loads(self.nlp.annotate(sentence, properties=self.props))\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_dict(_tokens):\n",
    "        tokens = defaultdict(dict)\n",
    "        for token in _tokens:\n",
    "            tokens[int(token['index'])] = {\n",
    "                'word': token['word'],\n",
    "                'lemma': token['lemma'],\n",
    "                'pos': token['pos'],\n",
    "                'ner': token['ner']\n",
    "            }\n",
    "        return tokens\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sNLP = StanfordNLP()\n",
    "    text = 'A blog post using Stanford CoreNLP Server. Visit www.khalidalnajjar.com for more details.'\n",
    "    print( \"Annotate:\", sNLP.annotate(text))\n",
    "    print( \"POS:\", sNLP.pos(text))\n",
    "    print( \"Tokens:\", sNLP.word_tokenize(text))\n",
    "    print( \"NER:\", sNLP.ner(text))\n",
    "    print( \"Parse:\", sNLP.parse(text))\n",
    "    print( \"Dep Parse:\", sNLP.dependency_parse(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This movie was actually neither that funny, nor super witty. The movie was meh. I liked watching that movie. If I had a choice, I would not watch that movie again.\"\n",
    "result = nlp.annotate(text,\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment, ner, pos',\n",
    "                       'outputFormat': 'json',\n",
    "                       'timeout': 1000,\n",
    "                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "res = nlp.annotate(\"I love you. I hate him. You are nice. He is dumb\",\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment',\n",
    "                       'outputFormat': 'json',\n",
    "                       'timeout': 1000,\n",
    "                   })\n",
    "for s in res[\"sentences\"]:\n",
    "    print(\"%d: '%s': %s %s\" % (\n",
    "        s[\"index\"],\n",
    "        \" \".join([t[\"word\"] for t in s[\"tokens\"]]),\n",
    "        s[\"sentimentValue\"], s[\"sentiment\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
